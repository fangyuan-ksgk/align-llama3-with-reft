{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\n",
    "  token=HF_TOKEN, # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import pyreft \n",
    "from pyreft import ReftModel\n",
    "from datasets import load_dataset\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "########################\n",
    "# Load Yi 1.5 6B model #\n",
    "########################\n",
    "\n",
    "# model_name_or_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_name_or_path = \"01-ai/Yi-1.5-6B\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "model_max_length = 4096\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=model_max_length, \n",
    "    padding_side=\"right\", use_fast=False)\n",
    "if \"Meta-Llama-3-\" in model_name_or_path:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "else:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "##################### \n",
    "# Load Reft adaptor #\n",
    "#####################\n",
    "\n",
    "reft_model = ReftModel.load(\"Ksgk-fy/Zalinger02_reft_llama3\", model, from_huggingface_hub=True)\n",
    "reft_model.set_device(\"cuda\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#   Run Inference   # \n",
    "#####################\n",
    "from patch import parse_positions # Patch comes to the rescue\n",
    "\n",
    "def run_inference(system_prompt, customer_query, conversation_history = \"\"):\n",
    "    # Data preparation\n",
    "    system_prompt = \"Role play as a FWD life insurance agent\"\n",
    "    system_prompt += \"[Conversation History] \" + \" \".join(conversation_history) + \"[End]\" \n",
    "    prompt = \"[Customer Utterance] \" + customer_query + \" [Sale Response] \"\n",
    "            \n",
    "    # tokenize and prepare the input\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}], \n",
    "        tokenize=False)\n",
    "    prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # get reft model configuration\n",
    "    reft_config = pyreft.ReftConfig(representations=[{\n",
    "        \"layer\": l, \"component\": \"block_output\",\n",
    "        \"low_rank_dimension\": 2,\n",
    "        \"intervention\": pyreft.LoreftIntervention(embed_dim=model.config.hidden_size,\n",
    "        low_rank_dimension=2)} for l in [8, 16, 24]])\n",
    "    share_weights = True # whether the prefix and suffix interventions sharing weights.\n",
    "    positions=\"f1+l1\"    # the intervening positions of prefix tokens (f[irst]1) and suffix tokens (l[ast]1).\n",
    "    first_n, last_n = parse_positions(positions)\n",
    "\n",
    "    unit_locations = torch.IntTensor([pyreft.get_intervention_locations(\n",
    "        last_position=prompt[\"input_ids\"].shape[-1], \n",
    "        first_n=first_n, \n",
    "        last_n=last_n,\n",
    "        pad_mode=\"last\",\n",
    "        num_interventions=len(reft_config.representations),\n",
    "        share_weights=share_weights\n",
    "    )]).permute(1, 0, 2).tolist()\n",
    "\n",
    "    _, reft_response = reft_model.generate(\n",
    "        prompt, unit_locations={\"sources->base\": (None, unit_locations)},\n",
    "        intervene_on_prompt=True, max_new_tokens=512, do_sample=True, \n",
    "        eos_token_id=terminators, early_stopping=True\n",
    "    )\n",
    "\n",
    "#############\n",
    "# Inference #\n",
    "#############\n",
    "\n",
    "customer_query = \"I am not sure if I need life insurance at this stage.\"\n",
    "conversation_history = [\"Customer: Hello I am Sarah\", \"Agent: Hi Sarah, I am John from FWD life insurance. How can I help you?\"]\n",
    "system_prompt = \"Role play as a FWD life insurance agent John\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
