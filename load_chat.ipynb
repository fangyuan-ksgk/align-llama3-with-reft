{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\n",
    "  token=HF_TOKEN, # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import pyreft \n",
    "from pyreft import ReftModel\n",
    "from datasets import load_dataset\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "########################\n",
    "# Load Llama3-8B model #\n",
    "########################\n",
    "\n",
    "model_name_or_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "model_max_length = 2048\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=model_max_length, \n",
    "    padding_side=\"right\", use_fast=False)\n",
    "if \"Meta-Llama-3-\" in model_name_or_path:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "else:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "##################### \n",
    "# Load Reft adaptor #\n",
    "#####################\n",
    "\n",
    "reft_model = ReftModel.load(\"Ksgk-fy/Zalinger02_reft_llama3\", model, from_huggingface_hub=True)\n",
    "reft_model.set_device(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
